{
  "tasks": {
    "A": {
      "name": "A",
      "status": "READY",
      "working_dir": null,
      "nexts": [
        "B"
      ],
      "prevs": [],
      "command": "\n# ========== Initial Setup ==========\nTASK=\"A\"\nWORKFLOW=\"DHT-Sensor-Capture-And-Preprocess\"\nMETRICS_TMP=\"/tmp/dagon_${TASK}.prom\"\nMETRICS_DIR=\"/var/lib/node_exporter/textfile_collector\"\nMETRICS_FINAL=\"${METRICS_DIR}/dagon_dht_${TASK}.prom\"\nMETRICS_BACKUP=\"/home/raspi/.dagon_metrics/dagon_dht_${TASK}.prom\"\n\nSTART_TS=$(date +%s.%N)\nSUCCESS=1\nRECORDS=0\n\nLOG_FILE=\"/home/raspi/dagon_capture_$(date +%Y%m%d_%H%M%S).log\"\nexec > >(tee -a \"$LOG_FILE\") 2>&1\n\necho \"==========================================\"\necho \"Starting DHT sensor capture\"\necho \"Date: $(date)\"\necho \"==========================================\"\n\n# --- Install pyserial ---\necho \"Installing pyserial...\"\npip install --no-cache-dir pyserial >/dev/null 2>&1\n\n# --- Inline Python script ---\npython3 << 'EOF'\nimport serial, re, json, time, os\n\n# Serial port configuration\nPORT = '/dev/ttyACM0'\nBAUDRATE = 9600\nDURATION = 30\nOUTPUT_FILE = 'output.json'\n\nprint(f\"Configuration:\\n  Port: {PORT}\\n  Duration: {DURATION}s\")\n\n# Check if port exists\nif not os.path.exists(PORT):\n    print(f\"ERROR: Port {PORT} not found\")\n    json.dump({'error': 'Port not found', 'timestamp': time.time()}, open(OUTPUT_FILE, 'w'), indent=2)\n    exit(1)\n\ntry:\n    # Initialize serial connection\n    ser = serial.Serial(PORT, BAUDRATE, timeout=2)\n    print(f\"Port {PORT} opened successfully\\n\")\n    print(\"Starting DHT11 sensor reading...\")\n    data, start = [], time.time()\n\n    # Main capture loop\n    while time.time() - start < DURATION:\n        line = ser.readline().decode('utf-8', errors='ignore').strip()\n        if not line:\n            continue\n        print(f\"{line}\")\n        # Extract temperature and humidity values\n        m = re.search(r'Humidity:\\s*([\\d.]+).*Temperature:\\s*([\\d.]+)', line)\n        if m:\n            data.append({'timestamp': time.time(), 'humidity': float(m[1]), 'temp_c': float(m[2])})\n\n    # Close connection and save data\n    ser.close()\n    print(f\"RECORDS_CAPTURED={len(data)}\")\n    json.dump(data, open(OUTPUT_FILE, 'w'), indent=2)\n    print(f\"Data saved to {OUTPUT_FILE} ({len(data)} records)\")\n\nexcept Exception as e:\n    # Handle errors\n    json.dump({'error': str(e), 'timestamp': time.time()}, open(OUTPUT_FILE, 'w'), indent=2)\n    print(f\"ERROR: {e}\")\n    exit(1)\n\nprint(\"Capture completed successfully\")\nEOF\n\n# --- Save results ---\nTIMESTAMP=$(date +%Y%m%d_%H%M%S)\nDEST=\"/home/raspi/sensor_output_${TIMESTAMP}.json\"\ncp output.json \"$DEST\" 2>/dev/null || echo '{\"error\":\"output.json not created\"}' > \"$DEST\"\n\necho \"==========================================\"\necho \"Log saved to: $LOG_FILE\"\necho \"Results saved to: $DEST\"\n\nEND_TS=$(date +%s.%N)\nDURATION=$(awk \"BEGIN {print ${END_TS}-${START_TS}}\")\n\n# Extract number of records\nRECORDS=$(grep -o \"RECORDS_CAPTURED=[0-9]*\" \"$LOG_FILE\" | tail -1 | cut -d= -f2)\nRECORDS=${RECORDS:-0}\n\n# Check for errors\ngrep -q \"ERROR\" \"$LOG_FILE\" && SUCCESS=0\n\n# Write metrics (ATOMIC)\ncat <<EOF_METRICS > ${METRICS_TMP}\n# HELP dagon_task_duration_seconds Duration of a Dagon task\n# TYPE dagon_task_duration_seconds gauge\ndagon_task_duration_seconds{workflow=\"${WORKFLOW}\",task=\"${TASK}\"} ${DURATION}\n\n# HELP dagon_task_success Task success (1=success, 0=failure)\n# TYPE dagon_task_success gauge\ndagon_task_success{workflow=\"${WORKFLOW}\",task=\"${TASK}\"} ${SUCCESS}\n\n# HELP dagon_task_records_total Records processed by task\n# TYPE dagon_task_records_total gauge\ndagon_task_records_total{workflow=\"${WORKFLOW}\",task=\"${TASK}\"} ${RECORDS}\nEOF_METRICS\n\n# Write metrics to backup location\nmkdir -p $(dirname ${METRICS_BACKUP})\nmv ${METRICS_TMP} ${METRICS_BACKUP}\necho \"Metrics saved to: ${METRICS_BACKUP}\"\n\n# Copy to Prometheus directory\nif [ -d \"${METRICS_DIR}\" ]; then\n    cp ${METRICS_BACKUP} ${METRICS_FINAL} 2>/dev/null && echo \"Metrics published to Prometheus: ${METRICS_FINAL}\" || echo \"Note: Could not publish to Prometheus (will retry outside container)\"\nfi\n\necho \"==========================================\"\n",
      "type": "remoteapptainertask"
    },
    "B": {
      "name": "B",
      "status": "READY",
      "working_dir": null,
      "nexts": [],
      "prevs": [
        "A"
      ],
      "command": "\n# ========== Preprocessing Setup ==========\nTASK=\"B\"\nWORKFLOW=\"DHT-Sensor-Capture-And-Preprocess\"\nMETRICS_TMP=\"/tmp/dagon_${TASK}.prom\"\nMETRICS_DIR=\"/var/lib/node_exporter/textfile_collector\"\nMETRICS_FINAL=\"${METRICS_DIR}/dagon_dht_${TASK}.prom\"\nMETRICS_BACKUP=\"/home/raspi/.dagon_metrics/dagon_dht_${TASK}.prom\"\n\nSTART_TS=$(date +%s.%N)\nSUCCESS=1\nRECORDS=0\nLOG_FILE=\"/home/raspi/dagon_preprocess_$(date +%Y%m%d_%H%M%S).log\"\nexec > >(tee -a \"$LOG_FILE\") 2>&1\n\necho \"==========================================\"\necho \"Starting preprocessing\"\necho \"Date: $(date)\"\necho \"==========================================\"\n\n# --- Install requirements ---\necho \"Installing pandas...\"\npip install --no-cache-dir pandas >/dev/null 2>&1\n\n# --- Inline Python script for preprocessing ---\npython3 << 'EOF'\nimport json, glob, os, time\nimport pandas as pd\n\n# Wait a bit to ensure file is fully written\ntime.sleep(2)\n\n# Search for the latest captured file\nfiles = sorted(glob.glob('/home/raspi/sensor_output_*.json'))\nfiles = [f for f in files if '_preprocessed' not in f]\n\nif not files:\n    print(\"ERROR: No sensor_output files found\")\n    exit(1)\n\ninput_file = files[-1]\noutput_file = input_file.replace('.json', '_preprocessed.json')\n\nprint(f\"Processing: {input_file}\")\n\n# Read data\nwith open(input_file) as f:\n    data = json.load(f)\n\n# If the file contains an error dict, abort\nif isinstance(data, dict) and 'error' in data:\n    print(f\"ERROR in input file: {data['error']}\")\n    exit(1)\n\ndf = pd.DataFrame(data)\n\nif df.empty or 'humidity' not in df or 'temp_c' not in df:\n    print(\"ERROR: No data to process or missing keys\")\n    exit(1)\n\nsummary = {\n    \"count\": int(len(df)),\n    \"mean_humidity\": float(df[\"humidity\"].mean()),\n    \"mean_temp_c\": float(df[\"temp_c\"].mean()),\n    \"min_humidity\": float(df[\"humidity\"].min()),\n    \"max_humidity\": float(df[\"humidity\"].max()),\n    \"min_temp_c\": float(df[\"temp_c\"].min()),\n    \"max_temp_c\": float(df[\"temp_c\"].max()),\n    \"start_time\": float(df[\"timestamp\"].min()),\n    \"end_time\": float(df[\"timestamp\"].max())\n}\n\nprint(f\"RECORDS_CAPTURED={summary['count']}\")\nprint(f\"  Mean Humidity: {summary['mean_humidity']:.2f}%\")\nprint(f\"  Mean Temp: {summary['mean_temp_c']:.2f}\u00b0C\")\n\nwith open(output_file, 'w') as out:\n    json.dump({\"summary\": summary, \"raw\": data}, out, indent=2)\n\nprint(f\"Preprocessed file saved to {output_file}\")\nprint(\"Preprocessing completed successfully\")\nEOF\n\necho \"==========================================\"\necho \"Log saved to: $LOG_FILE\"\nEND_TS=$(date +%s.%N)\nDURATION=$(awk \"BEGIN {print ${END_TS}-${START_TS}}\")\n\n# Extract number of records\nRECORDS=$(grep -o \"RECORDS_CAPTURED=[0-9]*\" \"$LOG_FILE\" | tail -1 | cut -d= -f2)\nRECORDS=${RECORDS:-0}\n\n# Check for errors\ngrep -q \"ERROR\" \"$LOG_FILE\" && SUCCESS=0\n\n# Write metrics (ATOMIC)\ncat <<EOF_METRICS > ${METRICS_TMP}\n# HELP dagon_task_duration_seconds Duration of a Dagon task\n# TYPE dagon_task_duration_seconds gauge\ndagon_task_duration_seconds{workflow=\"${WORKFLOW}\",task=\"${TASK}\"} ${DURATION}\n\n# HELP dagon_task_success Task success (1=success, 0=failure)\n# TYPE dagon_task_success gauge\ndagon_task_success{workflow=\"${WORKFLOW}\",task=\"${TASK}\"} ${SUCCESS}\n\n# HELP dagon_task_records_total Records processed by task\n# TYPE dagon_task_records_total gauge\ndagon_task_records_total{workflow=\"${WORKFLOW}\",task=\"${TASK}\"} ${RECORDS}\nEOF_METRICS\n\n# Write metrics to backup location\nmkdir -p $(dirname ${METRICS_BACKUP})\nmv ${METRICS_TMP} ${METRICS_BACKUP}\necho \"Metrics saved to: ${METRICS_BACKUP}\"\n\n# Copy to Prometheus directory\nif [ -d \"${METRICS_DIR}\" ]; then\n    cp ${METRICS_BACKUP} ${METRICS_FINAL} 2>/dev/null && echo \"Metrics published to Prometheus: ${METRICS_FINAL}\" || echo \"Note: Could not publish to Prometheus (will retry outside container)\"\nfi\n\necho \"==========================================\"\n",
      "type": "remoteapptainertask"
    }
  },
  "name": "DHT-Sensor-Capture-And-Preprocess",
  "id": 0,
  "host": "localhost"
}